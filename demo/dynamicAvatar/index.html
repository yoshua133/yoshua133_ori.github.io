<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DynamicAvatar</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
        }
        header {
            text-align: center;
            padding-bottom: 5 px;
        }
        h1 {
            margin-bottom: 5px;
        }
        .container {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            margin: 10px;
        }
        .item {
            margin-left: 200px;
            margin-right: 200px;
            margin-bottom: 10px;
            box-sizing: border-box;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            /* text-align: center; */
        }
        .large_item {
            margin-left: 50px;
            margin-right: 50px;
            margin-bottom: 10px;
            box-sizing: border-box;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            /* text-align: center; */
        }
        .two-columns {
            display: flex;
            flex-wrap: wrap;
        }
        .column {
            width: calc(50% - 20px);
            max-height: 500px;
            margin: 10px;
        }
        .video_column {
            max-width: calc(50% - 20px);
            max-height: 400px;
            margin-left: 100px;
            margin-right: 100px;
            margin-bottom: 10px;
            padding: 10px;
        }
        .video_large {
            /* max-width: calc(50% - 20px); */
            max-height: 300px;
            margin-left: 100px;
            margin-right: 100px;
            margin-bottom: 10px;
            padding: 10px;
        }
        .small_img {
            display: flex;
            max-width: 40%;
            max-height: 700px;
            height: auto;
            padding: 50px; /* Add padding around the edges of the images */
            box-sizing: border-box; 
        }
        .large_img {
            max-width: 100%;
            height: auto;
            max-height: 800px; /*  Set the maximum height for the images */
            object-fit: cover; /* Crop the images if their aspect ratio doesn't fit the given dimensions */
            padding: 50px; /* Add padding around the edges of the images */
            box-sizing: border-box; /* Include padding and border in an element's total width and height */
            display: flex;
        }
        video {
            max-width: 100%;
            height: auto;
        }
        .media-container {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
        }
        .reference {
            font-size: 0.8em;
            vertical-align: super;
            display: inline;
        }


    </style>
</head>
<body>
    <header>
        <h1>DynamicAvatar</h1>
        <h5> <em>--Drive your realistic avatar using text prompt</em></h5>
        <p>Dawei Xiang</p>
    </header>
    <div class="container">
        <div class="item">
            <h2>Abstract</h2>
            We introduce a text-to-human-avatar pipeline, DynamicAvatar. Our model could generate realistic human avatar motion from text prompt. 
            This pipeline consists of two primary stages: first, we employ a text-to-motion model to create a human motion sequence 
            derived from a brief text description; second, we reconstruct a human avatar using a monocular video capturing 
            an individual's appearance. Our innovative Render Net significantly minimizes distortion 
            resulting from monotonous training data when compared to the baseline model[2].
            <!-- The input are a self-rotating video and a text prompt to describe a motion. 
            We reconstruct the human avatar from the self-rotating video and generate a sequence of human motion from the text prompt. 
            Then we drive the human avatar to do the motions. -->
        </div>
        <div class="item">
            <h2>Pipeline</h2>
                    <img class="large_img" src="./pipeline.png" alt="Photo abs">
        </div>
        
        <div class="item">
            
            <h2>Introduction</h2>
            Human avatar reconstruction has vast applications in fields such as VR/AR, the metaverse, film, and 
            video game production. The primary goal is to create a 3D reconstruction of a person using monocular or 
            multi-view video. Once the 3D human model is reconstructed, a key application involves 
            animating the model to perform specific actions.
            
            However, for individuals with little or no 3D animation experience, manually manipulating the 
            avatar's movements can be challenging. Our solution addresses this issue by generating motion sequences 
            based on text prompts and animating the reconstructed avatar accordingly. This approach allows users 
            without prior knowledge to effortlessly create customized 3D animations featuring their avatars.

        </div>

        <div class="item">
            <h2>Method</h2>
            The whole pipeline is composed with two parts: 1. text-to-motion and 2. avatar reconstruction. For the first part, we used
            Motion Diffusion model from [1]. For the second part, we have designed a new Rendering model based on SelfRecon[2].
            <video class="video_large"  controls loop autoplay muted>
                <source src="./text2motion.mov" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video class="video_large"  controls loop autoplay muted>
                <source src="./reconstruction.mov" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p> 
                Because in the training video, the model mainly 
                learnt how the predict the color along horizontal direction. 

                This results in an unbalanced distribution of the "viewing direction". And it concentrates on a horizontal direction. 
                Consequently, when the model is employed to predict from a different view direction which doesn't appear in training set,
                it tends to produce color distortions.
            </p>

            <p> 
                To rectify this issue, we have proposed a dynamic weighted render net. 
                This innovative approach significantly mitigates color distortion. 
                The subsequent section presents a comparison with the baseline model.
            </p>
        </div>
        <h2>Comparison with baseline</h2>
        <div class="media-container">
            
            <video class="video_column"  controls loop autoplay muted>
                <source src="./compare.mov" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <img class="small_img" src="./image_compare.png" alt="Photo abs">
        </div>
        <div class="large_item">
            <h2>Demo videos</h2>

            <div class="two-columns">
                <video class="video_column"  controls loop autoplay muted>
                    <source src="./chacha.mov" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <video class="video_column"  controls loop autoplay muted>
                    <source src="./dance_spin.mov" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
            <div class="two-columns">
                <video class="video_column"  controls loop autoplay muted>
                    <source src="./walk_around.mov" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <video class="video_column"  controls loop autoplay muted>
                    <source src="./chn_dance.mov" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>



            <!-- <video controls>
                <source src="./chacha.mov" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video controls>
                <source src="path/to/your/video2.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video> -->
        </div>
        <section>
            <h2>References</h2>
            <ol>
              <li id="ref1">
                Human Motion Diffusion Model. 
                Tevet, Guy and Raab, Sigal and Gordon, Brian and Shafir, Yonatan and Bermano, Amit H and Cohen-Or, Daniel. ICML 2023.
              </li>
              <li id="ref2">
                SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video. 
                Boyi Jiang and Yang Hong and Hujun Bao and Juyong Zhang. CVPR 2022.
            </li>
            </ol>
        </section>
    </div>
    

      
</body>
</html>
